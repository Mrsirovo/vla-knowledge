# VLA 发展时间线

> 概览 Vision-Language-Action 模型在机器人操作中的主要发展节点。

- **2022 — SayCan**
  - LLM 用于语言推理与步骤规划，结合技能库进行执行。首次把语言推理与低层技能结合到真实机器人任务。  

- **2022 — RT-1**
  - 提出 Robotics Transformer，在多任务真实机器人演示上训练端到端 Transformer 策略，展现规模化可行性。  

- **2023 — RT-2**
  - 将互联网 VLM 的语义知识迁移到机器人控制，显著增强零样本与概念泛化能力。  

- **2023 — Open X-Embodiment / RT-X**
  - 跨 22 种机器人、21 机构汇聚百万级演示，验证跨机器人正迁移。  

- **2024 — Octo**
  - 开放的通用策略（约 80 万轨迹），支持语言指令与目标图像双输入；强调快速微调。  

- **2024 — DROID 数据集**
  - in-the-wild 操作数据，564 场景，350 小时，强调多样性与泛化。  

- **2025 — OpenVLA**
  - 开源 7B VLA，基于 97 万真实演示，支持参数高效微调（LoRA 等），在消费级 GPU 上可用。  

- **2025 — 高效推理方法**
  - VLA-Cache、SP-VLA、EfficientVLA 等提出时序/空间维度的 token 复用与剪枝，加速部署。  

---
