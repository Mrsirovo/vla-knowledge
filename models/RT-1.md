# RT-1

## 背景
在 2022 年之前，机器人研究普遍依赖 **小规模单任务数据集** 和 **专用控制策略**，难以泛化到复杂多任务环境。自然语言能够描述任务目标，但如何与感知输入（图像）和低层控制动作结合，仍是一个开放问题。  
Google Robotics 团队提出 **Robotics Transformer (RT-1)**，目标是验证：是否可以借鉴 NLP 的 **大规模数据 + Transformer 架构**范式，在机器人操作中获得更强的泛化能力。

---

## 方法与架构
- **输入**：
  - **视觉**：机器人头部相机捕获的 RGB 图像帧。  
  - **语言**：自然语言指令（如 “pick up the red cup”）。  
- **模型架构**：
  - **视觉编码器**：使用 **EfficientNet** 将图像编码为特征。  
  - **语言编码器**：将文本指令嵌入到 Transformer 可处理的向量空间。  
  - **融合模块**：基于 **Transformer 编码器-解码器** 架构，将视觉和语言特征融合。  
  - **动作输出**：采用 **离散 token 化的动作空间**，即将低层控制信号（例如末端执行器位置变化）量化为有限 token，再通过自回归预测。  
- **训练目标**：
  - 监督学习：最小化预测动作序列与人类演示动作的差异。  

---

## 技术细节
- **数据规模**：约 **130,000+** 真实机器人演示，由 Google 的 Everyday Robots 平台收集。  
- **任务多样性**：覆盖数百种 manipulation 任务（抓取、搬运、开门、按按钮等）。  
- **动作表示**：6DoF 末端执行器姿态 + gripper 控制，被离散化为 token。  
- **优化方法**：使用标准自回归训练，基于 cross-entropy 损失。  
- **性能评估**：
  - 单一任务上，性能与专门训练的模型相当。  
  - 多任务混合训练时，模型能够在 **未见过的指令**上泛化，成功率显著高于传统模仿学习方法。  

---

## 局限性
- **单一机器人平台**：所有演示数据均来自 Google 的 Daily Robots，缺乏跨硬件验证。  
- **动作空间受限**：离散 token 化存在精度损失，影响复杂操作的稳定性。  
- **计算成本**：训练和推理均需要较强算力，难以在资源有限的实验室复现。  
- **泛化不足**：虽然能处理组合任务，但在完全新颖的任务场景中仍然失败率较高。  

---

## 意义
- **范式转变**：首次将 **Transformer 架构 + 大规模真实机器人数据** 结合，标志着机器人学习迈入“大模型时代”。  
- **可扩展性验证**：证明了“数据规模 → 泛化性能”的正相关关系，为后续 RT-2、RT-X 奠定了方法论基础。  
- **启发性贡献**：RT-1 启发了 **开源化与跨平台数据共享** 的后续工作，例如 Open X-Embodiment 与 Octo。  

---

## 参考文献
- Brohan et al., *RT-1: Robotics Transformer for Real-World Control at Scale*, RSS 2023.  
