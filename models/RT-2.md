# RT-2

## 背景
RT-1 已经展示了“大规模真实机器人演示 + Transformer”能够提升泛化，但仍然受限于：
- 数据规模有限（仅 130k 演示）。  
- 知识边界受制于机器人数据本身，无法理解超出训练集的语义。  

与此同时，大规模 **视觉-语言模型 (VLM)**（如 PaLM-E、Flamingo）证明了在 **互联网级图文数据**上预训练，可以学习强大的跨模态语义知识。  
**RT-2 的核心动机**：将 VLM 的“世界知识”迁移到机器人，提升零样本泛化能力。

---

## 方法与架构
- **输入**：
  - 自然语言任务指令（例如 “pick up the Pikachu toy”）。  
  - 环境视觉观测（RGB 图像）。  
- **模型架构**：
  - **视觉编码**：基于预训练的 VLM 模块提取图像语义特征。  
  - **语言编码**：Transformer 编码文本输入。  
  - **融合与动作解码**：利用 Transformer 架构，将语义知识与机器人演示数据结合，输出动作 token 序列。  
- **训练方式**：
  - **双源训练**：
    1. **互联网级图文对**：提供广泛的语义知识。  
    2. **机器人演示数据**：提供真实感知-行动映射。  
  - 通过联合训练，使模型既具备世界知识，又能执行低层控制。  

---

## 技术细节
- **数据规模**：
  - 包含 **数亿级网页图文对**（类似于 VLM 预训练语料）。  
  - 融合 **数十万条机器人演示数据**。  
- **动作表示**：
  - 延续 RT-1 的离散 token 化动作输出方式（末端执行器 6DoF + gripper 控制）。  
- **训练策略**：
  - 自回归预测动作 token。  
  - 多任务混合训练（网页理解 + 机器人控制）。  
- **能力提升**：
  - 支持语义泛化：能执行训练中未出现的任务（如识别卡通形象、符号化物体）。  
  - 零样本泛化：能根据自然语言执行“新概念”任务。  

---

## 局限性
- **计算成本极高**：同时处理 VLM 数据和机器人数据，需要大规模计算资源。  
- **跨模态对齐难题**：互联网图文知识与机器人感知存在 domain gap，对齐不完美。  
- **依赖大规模高质量数据**：网页数据噪声较多，可能影响控制精度。  
- **推理延迟**：7B 以上模型在真实机器人上推理较慢。  

---

## 意义
- **知识迁移**：首次证明 **互联网知识 → 机器人行动** 的可行性。  
- **零样本能力**：突破机器人学习依赖封闭任务集的限制，使机器人能理解并执行“未见过”的自然语言任务。  
- **范式推进**：RT-2 把机器人研究从“数据驱动模仿学习”推进到“知识增强的泛化策略”。  
- **后续影响**：直接启发了跨平台数据共享（RT-X）、开源通用模型（Octo）、以及参数高效微调 (LoRA) 在 OpenVLA 的应用。  

---

## 参考文献
- Brohan et al., *RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robots*, arXiv 2023.  
